{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "En2vX4FuwHlu"
   },
   "source": [
    "## Tensorflow ve Keras'a Giriş\n",
    "\n",
    "> Bu not defteri, [Yeni Başlayanlar İçin YZ Müfredatı](http://github.com/microsoft/ai-for-beginners)'nın bir parçasıdır. Eksiksiz öğrenme materyalleri kümesi için kod deposunu ziyaret edin.\n",
    "\n",
    "### Sinirsel Çerçeveler\n",
    "\n",
    "Sinir ağlarını eğitmek için şunlara ihtiyacınız olduğunu öğrendik:\n",
    "* Matrisleri hızla çarpmalısınız (tensörler).\n",
    "* Gradyan inişi optimizasyonunu gerçekleştirmek için gradyanları hesaplamalısınız.\n",
    "\n",
    "Sinir ağı çerçevelerinin yapmanıza izin verdikleri şunlardır:\n",
    "* Kullanılabilir herhangi bir hesaplamada, CPU veya GPU'da ve hatta TPU'da tensörlerle çalışırsınız.\n",
    "* Gradyanları otomatik olarak hesaplarsınız (tüm yerleşik tensör işlevleri için açıkça programlanmıştır).\n",
    "\n",
    "İsteğe bağlı olarak:\n",
    "* Sinir Ağı kurucusu / daha üst seviye APIler (ağı bir dizi katman olarak tanımlayın).\n",
    "* Basit eğitim işlevleri (Scikit Learn'de olduğu gibi `fit`)\n",
    "* Gradyan inişine ek olarak bir dizi optimizasyon (eniyileme) algoritması.\n",
    "* Veri işleme soyutlamaları (bu ideal olarak GPU'da da çalışacaktır)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cACQoFMwHl3"
   },
   "source": [
    "### En Popüler Çerçeveler\n",
    "\n",
    "* Tensorflow 1.x - yaygın olarak kullanılabilen ilk çerçevedir (Google). Statik hesaplama çizgesini tanımlamaya, GPU'ya göndermeye ve açıkça değerlendirmeye izin verir.\n",
    "* PyTorch - Facebook'tan popülaritesi artan bir çerçevedir.\n",
    "* Keras - sinir ağlarını kullanarak birleştirmek ve basitleştirmek için Tensorflow/PyTorch'un üstünde daha üst seviye API'dir (Francois Chollet).\n",
    "* Tensorflow 2.x + Keras - **dinamik hesaplama çizgesini** destekleyen ve numpy'ya (ve PyTorch'a) çok benzer tensör işlemlerini gerçekleştirmeye olanak tanıyan tümleşik Keras işlevselliğine sahip Tensorflow'un yeni sürümüdür.\n",
    "\n",
    "Tensorflow 2.x ve Keras'ı ele alacağız. Tensorflow'un 2.x.x sürümünün kurulu olduğundan emin olun:\n",
    "```\n",
    "pip install tensorflow\n",
    "```\n",
    "veya\n",
    "```\n",
    "conda install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xwqVx9-bwHl3",
    "outputId": "2aa591b4-b647-441f-9c8e-4e0da2d517a0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tp2xGV7wHl4"
   },
   "source": [
    "## Temel Kavramlar: Tensör\n",
    "\n",
    "**Tensör** çok boyutlu bir dizilimdir. Farklı veri türlerini temsil etmek için tensörleri kullanmak çok uygundur:\n",
    "* 400x400 - siyah beyaz resim\n",
    "* 400x400x3 - renkli resim\n",
    "* 16x400x400x3 - 16 renkli resimden oluşan minigrup\n",
    "* 25x400x400x3 - 25 fps'lik videonun bir saniyesi\n",
    "* 8x25x400x400x3 - 8 adet 1 saniyelik videodan oluşan minigrup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qG2bsaR7wHl4"
   },
   "source": [
    "### Simple Tensors\n",
    "\n",
    "You can easily create simple tensors from lists of np-arrays, or generate random ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ybpnk08HwHl4",
    "outputId": "fad9ed4a-df82-44a0-84ea-324bc71ea46f"
   },
   "outputs": [],
   "source": [
    "a = tf.constant([[1,2],[3,4]])\n",
    "print(a)\n",
    "a = tf.random.normal(shape=(10,3))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXFMsV3r09Ux"
   },
   "source": [
    "You can use arithmetic operations on tensors, which are performed element-wise, as in numpy. Tensors are automatically expanded to required dimension, if needed. To extract numpy-array from tensor, use `.numpy()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5Nu5Xgj1DnQ",
    "outputId": "0dfc8758-4ffd-4968-c7bf-6ba8d435df2e"
   },
   "outputs": [],
   "source": [
    "print(a-a[0])\n",
    "print(tf.exp(a)[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQ5zN6cVyrG7"
   },
   "source": [
    "## Variables\n",
    "\n",
    "Variables are useful to represent tensor values that can be modified using `assign` and `assign_add`. They are often used to represent neural network weights.\n",
    "\n",
    "As an example, here is a silly way to get a sum of all rows of tensor `a`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7pu0UZ-_yqfB",
    "outputId": "6708c83e-02e6-4442-8757-45918eb1fbc2"
   },
   "outputs": [],
   "source": [
    "s = tf.Variable(tf.zeros_like(a[0]))\n",
    "for i in a:\n",
    "  s.assign_add(i)\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIh1EHcezlNo"
   },
   "source": [
    "Much better way to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQIdWZ1kzn6P",
    "outputId": "1c123d9a-ecd2-4f2e-828e-5ade85ac8f63"
   },
   "outputs": [],
   "source": [
    "tf.reduce_sum(a,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-auwezDwHl6"
   },
   "source": [
    "## Computing Gradients\n",
    "\n",
    "For back propagation, you need to compute gradients. This is done using `tf.GradientTape()` idiom:\n",
    " * Add `with tf.GradientTape` block around our computations\n",
    " * Mark those tensors with respect to which we need to compute gradients by calling `tape.watch` (all variables are watched automatically)\n",
    " * Compute whatever we need (build computational graph)\n",
    " * Obtain gradients using `tape.gradient` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m8vFOXr7wHl6",
    "outputId": "860ac72e-50c7-4ff2-f258-747f27194f90"
   },
   "outputs": [],
   "source": [
    "a = tf.random.normal(shape=(2, 2))\n",
    "b = tf.random.normal(shape=(2, 2))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  tape.watch(a)  # Start recording the history of operations applied to `a`\n",
    "  c = tf.sqrt(tf.square(a) + tf.square(b))  # Do some math using `a`\n",
    "  # What's the gradient of `c` with respect to `a`?\n",
    "  dc_da = tape.gradient(c, a)\n",
    "  print(dc_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sfjBMBu59B5"
   },
   "source": [
    "## Example 1: Linear Regression\n",
    "\n",
    "Now we know enough to solve the classical problem of **Linear regression**. Let's generate small synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j723455WwHl7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "WJNK_J6v6I-Z",
    "outputId": "eb4a66a6-6b9a-4c8a-bc24-d81eeb2d3f27"
   },
   "outputs": [],
   "source": [
    "np.random.seed(13) # pick the seed for reproducability - change it to explore the effects of random variations\n",
    "\n",
    "train_x = np.linspace(0, 3, 120)\n",
    "train_labels = 2 * train_x + 0.9 + np.random.randn(*train_x.shape) * 0.5\n",
    "\n",
    "plt.scatter(train_x,train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ng4rZmGc6oxk"
   },
   "source": [
    "Linear regression is defined by a straight line $f_{W,b}(x) = Wx+b$, where $W, b$ are model parameters that we need to find. An error on our dataset $\\{x_i,y_u\\}_{i=1}^N$ (also called **loss function**) can be defined as mean square error:\n",
    "$$\n",
    "\\mathcal{L}(W,b) = {1\\over N}\\sum_{i=1}^N (f_{W,b}(x_i)-y_i)^2\n",
    "$$\n",
    "\n",
    "Let's define our model and loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxhI4GlB6aiH"
   },
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "output_dim = 1\n",
    "learning_rate = 0.1\n",
    "\n",
    "# This is our weight matrix\n",
    "w = tf.Variable([[100.0]])\n",
    "# This is our bias vector\n",
    "b = tf.Variable(tf.zeros(shape=(output_dim,)))\n",
    "\n",
    "def f(x):\n",
    "  return tf.matmul(x,w) + b\n",
    "\n",
    "def compute_loss(labels, predictions):\n",
    "  return tf.reduce_mean(tf.square(labels - predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUxwj3367gD2"
   },
   "source": [
    "We will train the model on a series of minibatches. We will use gradient descent, adjusting model parameters using the following formulae:\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "W^{(n+1)}=W^{(n)}-\\eta\\frac{\\partial\\mathcal{L}}{\\partial W} \\\\\n",
    "b^{(n+1)}=b^{(n)}-\\eta\\frac{\\partial\\mathcal{L}}{\\partial b} \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-991PErM7fJU"
   },
   "outputs": [],
   "source": [
    "def train_on_batch(x, y):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = f(x)\n",
    "    loss = compute_loss(y, predictions)\n",
    "    # Note that `tape.gradient` works with a list as well (w, b).\n",
    "    dloss_dw, dloss_db = tape.gradient(loss, [w, b])\n",
    "  w.assign_sub(learning_rate * dloss_dw)\n",
    "  b.assign_sub(learning_rate * dloss_db)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idr2VEWb9rr0"
   },
   "source": [
    "Let's do the training. We will do several passes through the dataset (so-called **epochs**), divide it into minibatches and call the function defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOuu0qpx-wAp"
   },
   "outputs": [],
   "source": [
    "# Shuffle the data.\n",
    "indices = np.random.permutation(len(train_x))\n",
    "features = tf.constant(train_x[indices],dtype=tf.float32)\n",
    "labels = tf.constant(train_labels[indices],dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zdIf6c_85Ht",
    "outputId": "43b04684-8b90-4c65-d5ff-20ebac61c73c"
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "for epoch in range(10):\n",
    "  for i in range(0,len(features),batch_size):\n",
    "    loss = train_on_batch(tf.reshape(features[i:i+batch_size],(-1,1)),tf.reshape(labels[i:i+batch_size],(-1,1)))\n",
    "  print('Epoch %d: last batch loss = %.4f' % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have obtained optimized parameters $W$ and $b$. Note that their values are similar to the original values used when generating the dataset ($W=2, b=1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "US6q0nCBD-LL",
    "outputId": "65a79620-a3eb-445b-aafb-60a60575ab0e"
   },
   "outputs": [],
   "source": [
    "w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "_e6xRMZFDnyI",
    "outputId": "d202b7fe-4383-4d82-b98e-a20f3180093e"
   },
   "outputs": [],
   "source": [
    "plt.scatter(train_x,train_labels)\n",
    "x = np.array([min(train_x),max(train_x)])\n",
    "y = w.numpy()[0,0]*x+b.numpy()[0]\n",
    "plt.plot(x,y,color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0giuwC9GHzi8"
   },
   "source": [
    "## Computational Graph and GPU Computations\n",
    "\n",
    "Whenever we compute tensor expression, Tensorflow builds a computational graph that can be computed on the available computing device, e.g. CPU or GPU. Since we were using arbitrary Python function in our code, they cannot be included as part of computational graph, and thus when running our code on GPU we would need to pass the data between CPU and GPU back and forth, and compute custom function on CPU.\n",
    "\n",
    "Tensorflow allows us to mark our Python function using `@tf.function` decorator, which will make this function a part of the same computational graph. This decorator can be applied to functions that use standard Tensorflow tensor operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HK7HPLz3Hyrl"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_on_batch(x, y):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = f(x)\n",
    "    loss = compute_loss(y, predictions)\n",
    "    # Note that `tape.gradient` works with a list as well (w, b).\n",
    "    dloss_dw, dloss_db = tape.gradient(loss, [w, b])\n",
    "  w.assign_sub(learning_rate * dloss_dw)\n",
    "  b.assign_sub(learning_rate * dloss_db)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7HusxWkGjLX"
   },
   "source": [
    "The code has not changed, but if you were running this code on GPU and on larger dataset - you would have noticed the difference in speed. \n",
    "\n",
    "## Dataset API\n",
    "\n",
    "Tensorflow contains a convenient API to work with data. Let's try to use it. We will also train our model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oYro9Lbr8q0M",
    "outputId": "78c0a6de-71bd-4eef-8819-439495b28672"
   },
   "outputs": [],
   "source": [
    "w.assign([[10.0]])\n",
    "b.assign([0.0])\n",
    "\n",
    "# Create a tf.data.Dataset object for easy batched iteration\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_x.astype(np.float32), train_labels.astype(np.float32)))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(256)\n",
    "\n",
    "for epoch in range(10):\n",
    "  for step, (x, y) in enumerate(dataset):\n",
    "    loss = train_on_batch(tf.reshape(x,(-1,1)), tf.reshape(y,(-1,1)))\n",
    "  print('Epoch %d: last batch loss = %.4f' % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A10prCPowHl7"
   },
   "source": [
    "## Example 2: Classification\n",
    "\n",
    "Now we will consider binary classification problem. A good example of such a problem would be a tumour classification between malignant and benign based on it's size and age.\n",
    "\n",
    "The core model is similar to regression, but we need to use different loss function. Let's start by generating sample data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0OTPkGpwHl7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0) # pick the seed for reproducibility - change it to explore the effects of random variations\n",
    "\n",
    "n = 100\n",
    "X, Y = make_classification(n_samples = n, n_features=2,\n",
    "                           n_redundant=0, n_informative=2, flip_y=0.05,class_sep=1.5)\n",
    "X = X.astype(np.float32)\n",
    "Y = Y.astype(np.int32)\n",
    "\n",
    "split = [ 70*n//100, (15+70)*n//100 ]\n",
    "train_x, valid_x, test_x = np.split(X, split)\n",
    "train_labels, valid_labels, test_labels = np.split(Y, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-_BjSHPwHl8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_dataset(features, labels, W=None, b=None):\n",
    "    # prepare the plot\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.set_xlabel('$x_i[0]$ -- (feature 1)')\n",
    "    ax.set_ylabel('$x_i[1]$ -- (feature 2)')\n",
    "    colors = ['r' if l else 'b' for l in labels]\n",
    "    ax.scatter(features[:, 0], features[:, 1], marker='o', c=colors, s=100, alpha = 0.5)\n",
    "    if W is not None:\n",
    "        min_x = min(features[:,0])\n",
    "        max_x = max(features[:,1])\n",
    "        min_y = min(features[:,1])*(1-.1)\n",
    "        max_y = max(features[:,1])*(1+.1)\n",
    "        cx = np.array([min_x,max_x],dtype=np.float32)\n",
    "        cy = (0.5-W[0]*cx-b)/W[1]\n",
    "        ax.plot(cx,cy,'g')\n",
    "        ax.set_ylim(min_y,max_y)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "tq0vFchQwHl8",
    "outputId": "9a5aa6a0-c92f-4d72-9e78-c0f615804bff",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_dataset(train_x, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Data\n",
    "\n",
    "Before training, it is common to bring our input features to the standard range of [0,1] (or [-1,1]). The exact reasons for that we will discuss later in the course, but in short the reason is the following. We want to avoid values that flow through our network getting too big or too small, and we normally agree to keep all values in the small range close to 0. Thus we initialize the weights with small random numbers, and we keep signals in the same range.\n",
    "\n",
    "When normalizing data, we need to subtract min value and divide by range. We compute min value and range using training data, and then normalize test/validation dataset using the same min/range values from the training set. This is because in real life we will only know the training set, and not all incoming new values that the network would be asked to predict. Occasionally, the new value may fall out of the [0,1] range, but that's not crucial.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_norm = (train_x-np.min(train_x)) / (np.max(train_x)-np.min(train_x))\n",
    "valid_x_norm = (valid_x-np.min(train_x)) / (np.max(train_x)-np.min(train_x))\n",
    "test_x_norm = (test_x-np.min(train_x)) / (np.max(train_x)-np.min(train_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjPlpf2-wHl8"
   },
   "source": [
    "## Training One-Layer Perceptron\n",
    "\n",
    "Let's use Tensorflow gradient computing machinery to train one-layer perceptron.\n",
    "\n",
    "Our neural network will have 2 inputs and 1 output. The weight matrix $W$ will have size $2\\times1$, and bias vector $b$ -- $1$.\n",
    "\n",
    "Core model will be the same as in previous example, but loss function will be a logistic loss. To apply logistic loss, we need to get the value of **probability** as the output of our network, i.e. we need to bring the output $z$ to the range [0,1] using `sigmoid` activation function: $p=\\sigma(z)$.\n",
    "\n",
    "If we get the probability $p_i$ for the i-th input value corresponding to the actual class $y_i\\in\\{0,1\\}$, we compute the loss as $\\mathcal{L_i}=-(y_i\\log p_i + (1-y_i)log(1-p_i))$. \n",
    "\n",
    "In Tensorflow, both those steps (applying sigmoid and then logistic loss) can be done using one call to `sigmoid_cross_entropy_with_logits` function. Since we are training our network in minibatches, we need to average out the loss across all elements of a minibatch using `reduce_mean`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdDxWeCqwHl8"
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random.normal(shape=(2,1)),dtype=tf.float32)\n",
    "b = tf.Variable(tf.zeros(shape=(1,),dtype=tf.float32))\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "@tf.function\n",
    "def train_on_batch(x, y):\n",
    "  with tf.GradientTape() as tape:\n",
    "    z = tf.matmul(x, W) + b\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y,logits=z))\n",
    "  dloss_dw, dloss_db = tape.gradient(loss, [W, b])\n",
    "  W.assign_sub(learning_rate * dloss_dw)\n",
    "  b.assign_sub(learning_rate * dloss_db)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAAgw0h6KzUd"
   },
   "source": [
    "We will use minibatches of 16 elements, and do a few epochs of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PfyqjVb2wHl8",
    "outputId": "308850b8-fe17-4cda-ac27-8bcda210f113"
   },
   "outputs": [],
   "source": [
    "# Create a tf.data.Dataset object for easy batched iteration\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_x_norm.astype(np.float32), train_labels.astype(np.float32)))\n",
    "dataset = dataset.shuffle(128).batch(2)\n",
    "\n",
    "for epoch in range(10):\n",
    "  for step, (x, y) in enumerate(dataset):\n",
    "    loss = train_on_batch(x, tf.expand_dims(y,1))\n",
    "  print('Epoch %d: last batch loss = %.4f' % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4_Atvn5K4K9"
   },
   "source": [
    "To make sure our training worked, let's plot the line that separates two classes. Separation line is defined by the equation $W\\times x + b = 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "PgRTHttLwHl9",
    "outputId": "e4407e1b-edf5-48e5-fdc2-da28120a3c6b"
   },
   "outputs": [],
   "source": [
    "plot_dataset(train_x,train_labels,W.numpy(),b.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our model behaves on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "oEQswfCGrmHw",
    "outputId": "3cf61882-60e1-4baa-8e51-0c31ea80875c"
   },
   "outputs": [],
   "source": [
    "pred = tf.matmul(test_x,W)+b\n",
    "fig,ax = plt.subplots(1,2)\n",
    "ax[0].scatter(test_x[:,0],test_x[:,1],c=pred[:,0]>0.5)\n",
    "ax[1].scatter(test_x[:,0],test_x[:,1],c=valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the accuracy on the validation data, we can cast boolean type to float, and compute the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUjdeIefsIsg",
    "outputId": "f267f505-8ba4-43ef-9ebe-df124c3c05a1"
   },
   "outputs": [],
   "source": [
    "tf.reduce_mean(tf.cast(((pred[0]>0.5)==test_labels),tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explain what goes on here:\n",
    "* `pred` is the values predicted by the network. They are not quite probabilities, because we have not used an activation function, but values greater than 0.5 correspond to class 1, and smaller - to class 0.\n",
    "*  `pred[0]>0.5` creates a boolean tensor of results, where `True` corresponds to class 1, and `False` - to class 0\n",
    "* We compare that tensor to expected labels `valid_labels`, getting the boolean vector or correct predictions, where `True` corresponds to the correct prediction, and `False` - to incorrect one.\n",
    "* We convert that tensor to floating point using `tf.cast`\n",
    "* We then compute the mean value using `tf.reduce_mean` - that is exactly our desired accuracy  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_95qF9lY2kHp"
   },
   "source": [
    "## Using TensorFlow/Keras Optimizers\n",
    "\n",
    "Tensorflow is closely integrated with Keras, which contains a lot of useful functionality. For example, we can use different **optimization algorithms**. Let's do that, and also print obtained accuracy during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ups7nlV22ofp",
    "outputId": "aa4dff06-82b9-4b2f-ca00-33970ea2b989"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "\n",
    "W = tf.Variable(tf.random.normal(shape=(2,1)))\n",
    "b = tf.Variable(tf.zeros(shape=(1,),dtype=tf.float32))\n",
    "\n",
    "@tf.function\n",
    "def train_on_batch(x, y):\n",
    "  vars = [W, b]\n",
    "  with tf.GradientTape() as tape:\n",
    "    z = tf.sigmoid(tf.matmul(x, W) + b)\n",
    "    loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(z,y))\n",
    "    correct_prediction = tf.equal(tf.round(y), tf.round(z))\n",
    "    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    grads = tape.gradient(loss, vars)\n",
    "    optimizer.apply_gradients(zip(grads,vars))\n",
    "  return loss,acc\n",
    "\n",
    "for epoch in range(20):\n",
    "  for step, (x, y) in enumerate(dataset):\n",
    "    loss,acc = train_on_batch(tf.reshape(x,(-1,2)), tf.reshape(y,(-1,1)))\n",
    "  print('Epoch %d: last batch loss = %.4f, acc = %.4f' % (epoch, float(loss),acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvAiaj_JndyP"
   },
   "source": [
    "**Task 1**: Plot the graphs of loss function and accuracy on training and validation data during training\n",
    "\n",
    "**Task 2**: Try to solve MNIST classificiation problem using this code. Hint: use `softmax_crossentropy_with_logits` or `sparse_softmax_cross_entropy_with_logits` as loss function. In the first case you need to feed expected output values in *one hot encoding*, and in the second case - as integer class number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "995iCprDrgYQ"
   },
   "source": [
    "## Keras\n",
    "### Deep Learning for Humans\n",
    "\n",
    "* Keras is a library originally developed by Francois Chollet to work on top of Tensorflow, CNTK and Theano, to unify all lower-level frameworks. You can still install Keras as a separate library, but it is not advised to do so. \n",
    "* Now Keras is included as part of Tensorflow library\n",
    "* You can easily construct neural networks from layers\n",
    "* Contains `fit` function to do all training, plus a lot of functions to work with typical data (pictures, text, etc.)\n",
    "* A lot of samples\n",
    "* Functional API vs. Sequential API\n",
    "\n",
    "Keras provides higher level abstractions for neural networks, allowing us to operate in terms of layers, models and optimizers, and not in terms of tensors and gradients.  \n",
    "\n",
    "Classical Deep Learning book from the creator of Keras: [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python)\n",
    "\n",
    "### Functional API\n",
    "\n",
    "When using functional API, we define the **input** to the network as `keras.Input`, and then compute the **output** by passing it through a series of computations. Finally, we define **model** as an object that transforms input into output.\n",
    "\n",
    "Once we obtained **model** object, we need to:\n",
    "* **Compile it**, by specifying loss function and the optimizer that we want to use with our model\n",
    "* **Train it** by calling `fit` function with the training (and possibly validation) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJWplVfy34Eo",
    "outputId": "9be976f2-4f9a-495c-bddc-a7f9ec30989a"
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(2,))\n",
    "z = tf.keras.layers.Dense(1,kernel_initializer='glorot_uniform',activation='sigmoid')(inputs)\n",
    "model = tf.keras.models.Model(inputs,z)\n",
    "\n",
    "model.compile(tf.keras.optimizers.Adam(0.1),'binary_crossentropy',['accuracy'])\n",
    "model.summary()\n",
    "h = model.fit(train_x_norm,train_labels,batch_size=8,epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "K2Kf60IrZcqs",
    "outputId": "b60b868d-3562-4715-f5d5-1f9764e45f09"
   },
   "outputs": [],
   "source": [
    "plt.plot(h.history['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJruFXmb_dur"
   },
   "source": [
    "### Sequential API\n",
    "\n",
    "Alternatively, we can start thinking of a model as of a **sequence of layers**, and just specify those layers by adding them to the `model` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iWc_kSr8_YXt",
    "outputId": "345dbe65-629d-468f-ed75-1d412c966340"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(5,activation='sigmoid',input_shape=(2,)))\n",
    "model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(tf.keras.optimizers.Adam(0.1),'binary_crossentropy',['accuracy'])\n",
    "model.summary()\n",
    "model.fit(train_x_norm,train_labels,validation_data=(test_x_norm,test_labels),batch_size=8,epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmHNhUU8bqEX"
   },
   "source": [
    "## Classification Loss Functions\n",
    "\n",
    "It is important to correctly specify loss function and activation function on the last layer of the network. The main rules are the following:\n",
    "* If the network has one output (**binary classification**), we use **sigmoid** activation function, for **multiclass classification** - **softmax**\n",
    "* If the output class is represented as one-hot-encoding, the loss function will be **cross entropy loss** (categorical cross-entropy), if the output contains class number - **sparse categorical cross-entropy**.  For **binary classification** - use **binary cross-entropy** (same as **log loss**)\n",
    "* **Multi-label classification** is when we can have an object belonging to several classes at the same time. In this case, we need to encode labels using one-hot encoding, and use **sigmoid** as activation function, so that each class probability is between 0 and 1.\n",
    "\n",
    "| Classification | Label Format | Activation Function | Loss |\n",
    "|---------------|-----------------------|-----------------|----------|\n",
    "| Binary      | Probability of 1st class | sigmoid | binary crossentropy |\n",
    "| Binary      | One-hot encoding (2 outputs) | softmax | categorical crossentropy |\n",
    "| Multiclass |  One-hot encoding | softmax | categorical crossentropy |\n",
    "| Multiclass | Class Number | softmax | sparse categorical crossentropy |\n",
    "| Multilabel | One-hot encoding | sigmoid | categorical crossentropy |\n",
    "\n",
    "> Binary classification can also be handled as a special case of multi-class classification with two outputs. In this case, we need to use **softmax**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZ-kWx84bMDH"
   },
   "source": [
    "**Task 3**: \n",
    "Use Keras to train MNIST classifier:\n",
    "* Notice that Keras contains some standard datasets, including MNIST. To use MNIST from Keras, you only need a couple of lines of code (more information [here](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/mnist))\n",
    "* Try several network configuration, with different number of layers/neurons, activation functions.\n",
    "\n",
    "What is the best accuracy you were able to achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yX6hqiafwHl9"
   },
   "source": [
    "## Takeaways\n",
    "\n",
    "* Tensorflow allows you to operate on tensors at low level, you have most flexibility.\n",
    "* There are convenient tools to work with data (`td.Data`) and layers (`tf.layers`)\n",
    "* For beginners/typical tasks, it is recommended to use **Keras**, which allows to construct networks from layers\n",
    "* If non-standard architecture is needed, you can implement your own Keras layer, and then use it in Keras models\n",
    "* It is a good idea to look at PyTorch as well and compare approaches. \n",
    "\n",
    "A good sample notebook from the creator of Keras on Keras and Tensorflow 2.0 can be found [here](https://t.co/k694J95PI8)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "IntroKerasTF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "livereveal": {
   "start_slideshow_at": "selected"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
