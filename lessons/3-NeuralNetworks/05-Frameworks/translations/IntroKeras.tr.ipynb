{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "En2vX4FuwHlu"
   },
   "source": [
    "## Keras ile Sinir Ağlarına En Basit Giriş\n",
    "\n",
    "> Bu not defteri, [Yeni Başlayanlar için YZ Müfredatı](http://github.com/microsoft/ai-for-beginners)'nın bir parçasıdır. Eksiksiz öğrenme materyalleri kümesi için kod deposunu ziyaret edin.\n",
    "\n",
    "### Sinir Çerçeveleri\n",
    "\n",
    "Sinir ağlarını eğitmek için çeşitli çerçeveler vardır. Ancak, hızlı bir başlangıç yapmak ve işlerin içeride nasıl çalıştığına dair fazla ayrıntıya girmek istemiyorsanız [Keras](https://keras.io/) kullanmayı düşünmelisiniz. Bu kısa eğitim başlamanıza yardımcı olacak ve işlerin nasıl yürüdüğünü daha iyi anlamak istiyorsanız - [Tensorflow ve Keras'a Giriş](IntroKerasTF.tr.ipynb) not defterine bakın."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cACQoFMwHl3"
   },
   "source": [
    "### İşleri hazırlamak\n",
    "\n",
    "Keras, Tensorflow 2.x çerçevesinin bir parçasıdır. Tensorflow'un 2.x.x sürümünün kurulu olduğundan emin olalım:\n",
    "```\n",
    "pip install tensorflow\n",
    "```\n",
    "veya\n",
    "```\n",
    "conda install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xwqVx9-bwHl3",
    "outputId": "2aa591b4-b647-441f-9c8e-4e0da2d517a0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "print(f'Tensorflow sürümü = {tf.__version__}')\n",
    "print(f'Keras sürümü\" = {keras.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tp2xGV7wHl4"
   },
   "source": [
    "## Temel Kavramlar: Tensör\n",
    "\n",
    "**Tensör** çok boyutlu bir dizidir. Farklı veri türlerini temsil etmek için tensör kullanmak çok uygundur:\n",
    "* 400x400 - siyah beyaz resim\n",
    "* 400x400x3 - renkli resim \n",
    "* 16x400x400x3 - 16 adet renkli resimden minigrup\n",
    "* 25x400x400x3 - 25 fps'lik videonun bir saniyesi\n",
    "* 8x25x400x400x3 - 8 adet 1 saniyelik videodan minigrup\n",
    "\n",
    "Tensörler, sinir ağı içindeki ağırlıkların yanı sıra, girdi/çıktı verilerini temsil etmek için de bize uygun bir yol sağlar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A10prCPowHl7"
   },
   "source": [
    "## Örnek Problem\n",
    "\n",
    "İkili sınıflandırma problemini ele alalım. Böyle bir soruna iyi bir örnek, boyutuna ve yaşına göre kötü ve iyi huylular arasında tümör sınıflandırması olabilir. Bazı örnek veriler oluşturarak başlayalım:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0OTPkGpwHl7"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0) # tekrarlanabilirlik için tohumu seçin - rastgele varyasyonların etkilerini keşfetmek için değiştirin\n",
    "\n",
    "n = 100\n",
    "X, Y = make_classification(n_samples = n, n_features=2,\n",
    "                           n_redundant=0, n_informative=2, flip_y=0.05,class_sep=1.5)\n",
    "X = X.astype(np.float32)\n",
    "Y = Y.astype(np.int32)\n",
    "\n",
    "split = [ 70*n//100 ]\n",
    "train_x, test_x = np.split(X, split)\n",
    "train_labels, test_labels = np.split(Y, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-_BjSHPwHl8"
   },
   "outputs": [],
   "source": [
    "def plot_dataset(features, labels, W=None, b=None):\n",
    "    # çizimi hazırlamak\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.set_xlabel('$x_i[0]$ -- (öznitelik 1)')\n",
    "    ax.set_ylabel('$x_i[1]$ -- (öznitelik 2)')\n",
    "    colors = ['r' if l else 'b' for l in labels]\n",
    "    ax.scatter(features[:, 0], features[:, 1], marker='o', c=colors, s=100, alpha = 0.5)\n",
    "    if W is not None:\n",
    "        min_x = min(features[:,0])\n",
    "        max_x = max(features[:,1])\n",
    "        min_y = min(features[:,1])*(1-.1)\n",
    "        max_y = max(features[:,1])*(1+.1)\n",
    "        cx = np.array([min_x,max_x],dtype=np.float32)\n",
    "        cy = (0.5-W[0]*cx-b)/W[1]\n",
    "        ax.plot(cx,cy,'g')\n",
    "        ax.set_ylim(min_y,max_y)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "tq0vFchQwHl8",
    "outputId": "9a5aa6a0-c92f-4d72-9e78-c0f615804bff"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_dataset(train_x, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verileri Normalleştirme\n",
    "\n",
    "Eğitimden önce, girdi özniteliklerimizi [0,1] (veya [-1,1]) standart aralığına getirmek yaygındır. Bunun tam nedenlerini kursun ilerleyen kısımlarında tartışacağız, ancak kısaca nedeni şudur. Ağımız üzerinden akan değerlerin çok büyük veya çok küçük olmasını önlemek istiyoruz ve normalde tüm değerleri 0'a yakın küçük bir aralıktaki tutmada hemfikiriz. Böylece ağırlıkları küçük rastgele sayılarla ilkliyoruz ve sinyalleri aynı değer aralığında tutuyoruz.\n",
    "\n",
    "Verileri normalleştirirken en küçük değeri çıkarmamız ve aralığa bölmemiz gerekiyor. Eğitim verilerini kullanarak en küçük değeri ve değer aralığını hesaplıyoruz ve ardından eğitim kümesindeki aynı minimum/aralık değerlerini kullanarak test/geçerleme veri kümesini normalleştiriyoruz. Bunun nedeni, gerçek hayatta sadece eğitim kümesini bileceğiz ve ağın tahmin etmesi istenecek gelen tüm yeni değerleri değil. Bazen yeni değer [0,1] aralığının dışına çıkabilir, ancak bu çok önemli değildir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_norm = (train_x-np.min(train_x,axis=0)) / (np.max(train_x,axis=0)-np.min(train_x,axis=0))\n",
    "test_x_norm = (test_x-np.min(train_x,axis=0)) / (np.max(train_x,axis=0)-np.min(train_x,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjPlpf2-wHl8"
   },
   "source": [
    "## Tek Katmanlı Ağ Eğitimi (Algılayıcı)\n",
    "\n",
    "Çoğu durumda, bir sinir ağı bir dizi katman olacaktır. Keras'ta `Sequential` (dizili) model kullanılarak aşağıdaki şekilde tanımlanabilir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.Input(shape=(2,)))\n",
    "model.add(keras.layers.Dense(1))\n",
    "model.add(keras.layers.Activation(keras.activations.sigmoid))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Burada önce modeli oluşturuyoruz ve ardından ona katmanlar ekliyoruz:\n",
    "* İlk `Input` (Girdi) katmanı (aslında katman diyemeyiz) ağın girdi boyutunun beyanını içerir.\n",
    "* `Dense` (yoğun) katman, eğitilebilir ağırlıkları içeren gerçek algılayıcıdır.\n",
    "* Son olarak, ağın sonucunu 0-1 aralığına getirmek (onu olasılık yapmak) için *sigmoid* `Activation` (etkinleştirme) işlevine sahip bir katman vardır.\n",
    "\n",
    "Girdi boyutu ve etkinleştirme işlevi, kısa olması için doğrudan `Dense` katmanında da belirtilebilir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(1,input_shape=(2,),activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeli eğitmeden önce, onu **derlememiz** gerekir, bu da esasen şunu belirtmek anlamına gelir:\n",
    "* **Kayıp fonksiyonu**, kaybın nasıl hesaplandığını tanımlar. İki sınıflı sınıflandırma problemimiz olduğu için *ikili çapraz entropi kaybı* kullanacağız.\n",
    "* **Optimizer (eniyileyici)** kullanmak için. En basit seçenek, *rasgele gradyan inişi* için `sgd`'yi kullanmaktır veya `adam` gibi daha karmaşık eniyileyicileri kullanabilirsiniz.\n",
    "* Eğitimimizin başarısını ölçmek için kullanmak istediğimiz **metrikler**. Sınıflandırma görevi olduğundan, iyi bir metrik `Accuracy` (doğruluk) (veya kısaca `acc`) olacaktır.\n",
    "\n",
    "Kaybı, metrikleri ve eniyileyiciyi dizgiler (string) olarak veya Keras çerçevesinden bazı nesneler sağlayarak belirtebiliriz. Örneğimizde, modelimizin öğrenme oranına ince ayar yapmak için `learning_rate` parametresini belirtmemiz gerekiyor ve bu nedenle Keras SGD eniyileyicisinin tam adını sağlıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.2),loss='binary_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeli derledikten sonra `fit` metodunu çağırarak asıl eğitimi yapabiliriz. En önemli parametreler şunlardır:\n",
    "* `x` ve `y` sırasıyla eğitim verilerini, öznitelikleri ve etiketleri belirtir.\n",
    "* Her dönemde geçerlemenin yapılmasını istiyorsak, bir dizi özellik ve etiket olacak olan `validation_data` parametresini belirtebiliriz.\n",
    "* `epochs`, dönemlerin sayısını belirtti.\n",
    "* Eğitimin minigruplarda gerçekleşmesini istiyorsak, `batch_size` parametresini belirtebiliriz. Ayrıca verileri `x`/`y`/`validation_data`'ya aktarmadan önce manuel olarak önceden toplu işleyebilirsiniz; bu durumda `batch_size` gerekmez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=train_x_norm,y=train_labels,validation_data=(test_x_norm,test_labels),epochs=10,batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4_Atvn5K4K9"
   },
   "source": [
    "Eğitimi nasıl etkilediklerini görmek için farklı eğitim parametreleriyle denemeler yapabilirsiniz:\n",
    "* `batch_size` ayarının çok büyük olması (veya hiç belirtilmemesi) daha az kararlı eğitime neden olabilir, çünkü düşük boyutlu verilerle küçük toplu iş boyutları her bir özel durum için gradyanın daha kesin yönünü sağlar.\n",
    "* Çok yüksek `learning_rate` (öğrenme oranı), aşırı öğrenme ile veya daha az kararlı sonuçlarla sonuçlanabilirken, çok düşük öğrenme oranı, sonuca ulaşmanın daha fazla dönem alacağı anlamına gelir.\n",
    "\n",
    "> Ağı daha fazla eğitmek için `fit` (oturt) işlevini arka arkaya birkaç kez çağırabileceğinizi unutmayın. Eğitime sıfırdan başlamak istiyorsanız - hücreyi model tanımıyla yeniden çalıştırmanız gerekir.\n",
    "\n",
    "Eğitimimizin işe yaradığından emin olmak için iki sınıfı ayıran çizgiyi çizelim. Ayırma çizgisi $W\\times x + b = 0.5$ denklemiyle tanımlanır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "PgRTHttLwHl9",
    "outputId": "e4407e1b-edf5-48e5-fdc2-da28120a3c6b"
   },
   "outputs": [],
   "source": [
    "plot_dataset(train_x,train_labels,model.layers[0].weights[0],model.layers[0].weights[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvAiaj_JndyP"
   },
   "source": [
    "## Eğitim grafiklerini çizme\n",
    "\n",
    "`fit` işlevi, sonuç olarak, her dönemdeki kaybı ve metrikleri gözlemlemek için kullanılabilen `history` (tarih) nesnesini döndürür. Aşağıdaki örnekte küçük bir öğrenme oranı ile eğitime yeniden başlayacağız ve kayıp ve doğruluğun nasıl davrandığını gözlemleyeceğiz.\n",
    "\n",
    "> `Sequential` modeli tanımlamak için biraz farklı sözdizimi kullandığımızı **unutmayın**. Katmanları tek tek eklemek (`add`) yerine, ilk etapta modeli oluştururken katmanların listesini de belirtebiliriz - bu biraz daha kısa sözdizimidir ve onu kullanmayı tercih edebilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(1,input_shape=(2,),activation='sigmoid')])\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.05),loss='binary_crossentropy',metrics=['acc'])\n",
    "hist = model.fit(x=train_x_norm,y=train_labels,validation_data=(test_x_norm,test_labels),epochs=10,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Çok Sınıflı Sınıflandırma\n",
    "\n",
    "Bir çok sınıflı sınıflandırma problemini çözmeniz gerekiyorsa, ağınızın birden fazla çıktısı olacaktır - ki $C$ sınıflarının sayısına karşılık gelir. Her çıktı belirli bir sınıfın olasılığını içerecektir.\n",
    "\n",
    "> Aynı şekilde ikili sınıflandırma gerçekleştirmek için iki çıktılı bir ağ da kullanabileceğinizi unutmayın. Şimdi tam olarak bunu göstereceğiz.\n",
    "\n",
    "Bir ağdan bir $p_1,\\dots, p_C$ olasılıkları kümesi çıktılamasını beklediğinizde, hepsinin toplamının 1'e eşit olmasına ihtiyacımız var. Bunu sağlamak için, son katmanda son etkinleştirme fonksiyonu olarak `softmax` kullanıyoruz. **Softmaks** bir vektör girdisi alır ve bu vektörün tüm bileşenlerinin olasılıklara dönüştürülmesini sağlar.\n",
    "\n",
    "Ayrıca, ağın çıktısı $C$ boyutlu bir vektör olduğundan, aynı forma sahip etiketlere ihtiyacımız var. Bu, $i$ sınıfının değeri, $i$. konumda 1 ile sıfırlardan oluşan bir vektöre dönüştürüldüğünde **bire bir kodlama** kullanılarak gerçekleştirilebilir.\n",
    "\n",
    "Sinir ağının olasılık çıktısını beklenen bire bir kodlanmış etiketle karşılaştırmak için **çapraz entropi kaybı** işlevini kullanırız. İki olasılık dağılımı alır ve ne kadar farklı olduklarının bir değerini verir.\n",
    "\n",
    "O halde $C$ sınıfı olan çok sınıflı sınıflandırma için yapmamız gerekenleri özetlemek gerekirse:\n",
    "* Ağın son katmanında $C$ adet nöronları olmalıdır.\n",
    "* Son etkinleştirme işlevi **softmaks** olmalıdır.\n",
    "* Kayıp, **çapraz entropi kaybı** olmalıdır.\n",
    "* Etiketler **bire bir kodlamaya** dönüştürülmelidir (bu, `numpy` kullanılarak veya Keras utils `to_categorical` kullanılarak yapılabilir)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(5,input_shape=(2,),activation='relu'),\n",
    "    keras.layers.Dense(2,activation='softmax')\n",
    "])\n",
    "model.compile(keras.optimizers.Adam(0.01),'categorical_crossentropy',['acc'])\n",
    "\n",
    "# Bire bir kodlamaya dönüştürmenin iki yolu\n",
    "train_labels_onehot = keras.utils.to_categorical(train_labels)\n",
    "test_labels_onehot = np.eye(2)[test_labels]\n",
    "\n",
    "hist = model.fit(x=train_x_norm,y=train_labels_onehot,\n",
    "                 validation_data=[test_x_norm,test_labels_onehot],batch_size=1,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seyrek Kategorik Çapraz Entropi\n",
    "\n",
    "Çok sınıflı sınıflandırmadaki etiketler genellikle sınıf numaralarıyla temsil edilir. Keras ayrıca, sınıf numarasının bire bir vektörler değil, tamsayılar olmasını bekleyen **seyrek kategorik çapraz entropi** adı verilen başka bir tür kayıp işlevini de destekler. Bu tür bir kayıp fonksiyonunu kullanarak eğitim kodumuzu basitleştirebiliriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(keras.optimizers.Adam(0.01),'sparse_categorical_crossentropy',['acc'])\n",
    "model.fit(x=train_x_norm,y=train_labels,validation_data=[test_x_norm,test_labels],batch_size=1,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Çok Etiketli Sınıflandırma\n",
    "\n",
    "> **Note** that this is very similar to using **different neural networks** to do binary classification for each particular class - only the initial part of the network (up to final classification layer) is shared for all classes.\n",
    "\n",
    "Bazen nesnelerimizin aynı anda iki sınıfa ait olabileceği durumlar olur. Örnek olarak, resimdeki kediler ve köpekler için bir sınıflandırıcı geliştirmek istediğimizi, ancak hem kedilerin hem de köpeklerin bulunduğu durumlara da izin vermek istediğimizi varsayalım.\n",
    "\n",
    "Çok etiketli sınıflandırma ile, bire bir kodlanmış vektör yerine, girdi örneğiyle ilgili tüm sınıflara karşılık gelen 1 konumunda bir vektöre sahip olacağız. Bu nedenle, ağın çıktısı tüm sınıflar için normalleştirilmiş olasılıklara sahip olmamalı, bunun yerine her sınıf için ayrı ayrı olmalıdır - bu, **sigmoid** etkinleştirme fonksiyonunun kullanılmasına karşılık gelir. Çapraz entropi kaybı hala bir kayıp fonksiyonu olarak kullanılabilir.\n",
    "\n",
    "> Bunun her bir sınıf için ikili sınıflandırma yapmak için **farklı sinir ağları** kullanmaya çok benzer olduğuna **dikkat edin** - tüm sınıflar için ağın yalnızca ilk kısmı (son sınıflandırma katmanına kadar) paylaşılır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmHNhUU8bqEX"
   },
   "source": [
    "## Sınıflandırma Kaybı Fonksiyonlarının Özeti\n",
    "\n",
    "Ağın son katmanındaki kayıp fonksiyonu ve etkinleştirme fonksiyonuna göre ikili, çok sınıflı ve çok etiketli sınıflandırmanın farklılık gösterdiğini gördük. Yeni öğrenmeye başlıyorsanız, biraz kafa karıştırıcı olabilir, ancak burada aklınızda bulundurmanız gereken birkaç kural vardır:\n",
    "* Eğer ağda bir tane çıktılı (**ikili sınıflandırma**) varsa **sigmoid** etkinleştirme işlevini kullanırız, **çok sınıflı sınıflandırma** içinse  **softmaks**.\n",
    "* Çıktı sınıfı bire bir kodlama olarak temsil edilirse, kayıp işlevi **çapraz entropi kaybı** (kategorik çapraz entropi) eğer çıktı sınıf numarası içeriyorsa **seyrek kategorik çapraz entropi**  olacaktır. **İkili sınıflandırma** için **ikili çapraz entropi** kullanın (**logaritmik kayıp** ile aynıdır).\n",
    "* **Çok etiketli sınıflandırma**, aynı anda birkaç sınıfa ait bir nesneye sahip olabileceğimiz zamandır. Bu durumda, etiketleri bire bir kodlama kullanarak kodlamamız ve etkinleştirme fonksiyonu olarak **sigmoid** kullanmamız gerekir, böylece her sınıf olasılığı 0 ile 1 arasında olur.\n",
    "\n",
    "| Sınıflandırma | Etiket Formatı | Etkinleştirme Fonksiyonu | Kayıp |\n",
    "|---------------|-----------------------|-----------------|----------|\n",
    "| İkili         | 1. sınıf olasılığı | sigmoid | ikili çapraz entropi |\n",
    "| İkili         | Bire bir kodlama (2 çıktılı) | softmax | kategorik çapraz entropi |\n",
    "| Çok sınıflı   | Bire bir kodlama | softmaks | kategorik çapraz entropi |\n",
    "| Çok sınıflı   | Sınıf sayısı | softmaks | seyrek kategorik çapraz entropi |\n",
    "| Çok sınıflı   | Bire bir kodlama | sigmoid | kategorik çapraz entropi |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZ-kWx84bMDH"
   },
   "source": [
    "**Görev**:\n",
    "MNIST el yazısı rakamları için bir sınıflandırıcı eğitmede Keras'ı kullanın:\n",
    "* Keras'ın MNIST dahil bazı standart veri kümeleri içerdiğine dikkat edin. MNIST'i Keras'tan kullanmak için yalnızca birkaç satır koda ihtiyacınız vardır (daha fazla bilgi [burada](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/mnist)dır).\n",
    "* Farklı sayıda katman/nöron, etkinleştirme işlevleriyle birkaç ağ yapılandırması deneyin.\n",
    "\n",
    "Ulaşabildiğiniz en iyi doğruluk nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yX6hqiafwHl9"
   },
   "source": [
    "## Ana Fikirler\n",
    "\n",
    "* **Keras**, yeni başlayanlar için gerçekten tavsiye edilir, çünkü katmanlardan ağları oldukça kolay bir şekilde oluşturmaya ve ardından sadece birkaç satır kodla eğitmeye olanak tanır.\n",
    "* Standart olmayan bir mimari gerekiyorsa, Tensorflow'u biraz daha derinlemesine öğrenmeniz gerekir. Veya birinden özel mantığı bir Keras katmanı olarak uygulamasını isteyebilir ve ardından bunu Keras modellerinde kullanabilirsiniz.\n",
    "* PyTorch'a da bakmak ve yaklaşımları karşılaştırmak iyi bir fikirdir.\n",
    "\n",
    "Keras üzerine Keras ve Tensorflow 2.0'nin yaratıcısından güzel bir örnek not defterini [burada](https://t.co/k694J95PI8) bulabilirsiniz."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "IntroKerasTF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "livereveal": {
   "start_slideshow_at": "selected"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
