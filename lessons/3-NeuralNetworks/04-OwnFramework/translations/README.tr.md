# Sinir AÄŸlarÄ±na GiriÅŸ. Ã‡ok KatmanlÄ± AlgÄ±layÄ±cÄ±

Ã–nceki bÃ¶lÃ¼mde, en basit sinir aÄŸÄ± modelini Ã¶ÄŸrendiniz - tek katmanlÄ± algÄ±layÄ±cÄ±, doÄŸrusal iki sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma modeli.

Bu bÃ¶lÃ¼mde, bu modeli daha esnek bir Ã§erÃ§eveye geniÅŸleterek ÅŸunlarÄ± yapmayÄ± saÄŸlayacaÄŸÄ±z:

* Ä°ki sÄ±nÄ±fa ek olarak **Ã§ok sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma** gerÃ§ekleÅŸtirme
* SÄ±nÄ±flandÄ±rmaya ek olarak **regresyon (baÄŸlanÄ±m) problemlerini** Ã§Ã¶zme
* DoÄŸrusal olarak ayrÄ±lamayan sÄ±nÄ±flarÄ± ayÄ±rma.

Python'da farklÄ± sinir aÄŸÄ± mimarileri oluÅŸturmamÄ±za izin verecek kendi modÃ¼ler Ã§erÃ§evemizi de geliÅŸtireceÄŸiz.

## [Ders Ã¶ncesi sÄ±navÄ±](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/104)

## Makine Ã–ÄŸrenmesinin FormÃ¼lleÅŸtirilmesi

Makine Ã–ÄŸrenmesi problemini formÃ¼lleÅŸtirerek baÅŸlayalÄ±m. **Y** etiketli bir **X** eÄŸitim veri kÃ¼memiz olduÄŸunu ve en doÄŸru tahminleri yapacak bir *f* modeli oluÅŸturmamÄ±z gerektiÄŸini varsayalÄ±m. Tahminlerin kalitesi **KayÄ±p iÅŸlevi** &lagran; ile Ã¶lÃ§Ã¼lÃ¼r. AÅŸaÄŸÄ±daki kayÄ±p fonksiyonlarÄ± sÄ±klÄ±kla kullanÄ±lÄ±r:

* BaÄŸlanÄ±m problemi iÃ§in, bir sayÄ±yÄ± tahmin etmemiz gerektiÄŸinde, **mutlak hata** &sum;<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| veya **kare hatasÄ±** &sum;<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i )</sup>)<sup>2</sup> kullanabiliriz.
* SÄ±nÄ±flandÄ±rma iÃ§in **0-1 kaybÄ±** (esas olarak modelin **doÄŸruluÄŸu** ile aynÄ±dÄ±r) veya **lojistik kayÄ±p** kullanÄ±rÄ±z.

Tek katmanlÄ± algÄ±layÄ±cÄ± iÃ§in *f* iÅŸlevi, *f(x)=wx+b* doÄŸrusal iÅŸlevi olarak tanÄ±mlanmÄ±ÅŸtÄ±r (burada *w* aÄŸÄ±rlÄ±k matrisidir, *x* girdi Ã¶zniteliklerinin vektÃ¶rÃ¼dÃ¼r ve *b* ek girdi vektÃ¶rÃ¼dÃ¼r). FarklÄ± sinir aÄŸÄ± mimarileri iÃ§in bu iÅŸlev daha karmaÅŸÄ±k bir biÃ§im alabilir.

> SÄ±nÄ±flandÄ±rma vakasÄ±nda, genellikle aÄŸ Ã§Ä±ktÄ±sÄ± olarak karÅŸÄ±lÄ±k gelen sÄ±nÄ±flarÄ±n olasÄ±lÄ±klarÄ±nÄ±n alÄ±nmasÄ± arzu edilir. Rastgele sayÄ±larÄ± olasÄ±lÄ±klara dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in (Ã¶rneÄŸin Ã§Ä±ktÄ±yÄ± normalleÅŸtirmek iÃ§in), genellikle **softmaks** iÅŸlevi &sigma;'yÄ± kullanÄ±rÄ±z ve *f* iÅŸlevi *f(x)=&sigma;(wx+b)* olur.

YukarÄ±daki *f* tanÄ±mÄ±nda *w* ve *b*, &theta;=âŸ¨*w,b*âŸ© **parametreler**i  olarak adlandÄ±rÄ±lÄ±r. Veri kÃ¼mesi âŸ¨**X**,**Y**âŸ© verildiÄŸinde, &theta; parametrelerinin bir fonksiyonu olarak tÃ¼m veri kÃ¼mesindeki genel bir hatayÄ± hesaplayabiliriz.

> âœ… **Sinir aÄŸÄ± eÄŸitiminin amacÄ±, &theta; parametrelerini deÄŸiÅŸtirerek hatayÄ± en aza indirmektir.**

## Gradyan Ä°niÅŸi Eniyilemesi

**Gradyan iniÅŸ**i adÄ± verilen iyi bilinen bir iÅŸlev eniyileme yÃ¶ntemi vardÄ±r. Buradaki fikir, parametrelere gÃ¶re kayÄ±p fonksiyonunun bir tÃ¼revini (Ã§ok boyutlu durumda **gradyan** olarak adlandÄ±rÄ±lÄ±r) hesaplayabilmemiz ve parametreleri, hatanÄ±n azalacaÄŸÄ± ÅŸekilde deÄŸiÅŸtirebilmemizdir. Bu, aÅŸaÄŸÄ±daki gibi formÃ¼le dÃ¶kÃ¼lebilir:

* Parametreleri bazÄ± rastgele deÄŸerlerle ilklet w<sup>(0)</sup>, b<sup>(0)</sup>
* AÅŸaÄŸÄ±daki adÄ±mÄ± birÃ§ok kez tekrarlayÄ±n:
     - w<sup>(i+1)</sup> = w<sup>(i)</sup>-&eta;&part;&lagran;/&part;w
     - b<sup>(i+1)</sup> = b<sup>(i)</sup>-&eta;&part;&lagran;/&part;b

EÄŸitim esnasÄ±nda, eniyileme adÄ±mlarÄ±nÄ±n tÃ¼m veri kÃ¼mesi dikkate alÄ±narak hesaplanmasÄ± gerekir (kaybÄ±n tÃ¼m eÄŸitim Ã¶rneklerinin toplamÄ± olarak hesaplandÄ±ÄŸÄ±nÄ± unutmayÄ±n). Bununla birlikte, gerÃ§ek hayatta **minigruplar** olarak adlandÄ±rÄ±lan veri kÃ¼mesinin kÃ¼Ã§Ã¼k kÄ±sÄ±mlarÄ±nÄ± alÄ±r ve bir veri alt kÃ¼mesine dayalÄ± olarak gradyanlarÄ± hesaplarÄ±z. Alt kÃ¼me her seferinde rastgele alÄ±ndÄ±ÄŸÄ±ndan, bu yÃ¶nteme **rasgele gradyan iniÅŸi** (SGD - RGÄ°) denir.

## Ã‡ok KatmanlÄ± AlgÄ±layÄ±cÄ±lar ve Geri Yayma

Tek katmanlÄ± aÄŸ, yukarÄ±da gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z gibi, doÄŸrusal olarak ayrÄ±labilir sÄ±nÄ±flarÄ± sÄ±nÄ±flandÄ±rma yeteneÄŸine sahiptir. Daha zengin bir model oluÅŸturmak iÃ§in aÄŸÄ±n birkaÃ§ katmanÄ±nÄ± birleÅŸtirebiliriz. Matematiksel olarak bu, *f* fonksiyonunun daha karmaÅŸÄ±k bir forma sahip olacaÄŸÄ± ve birkaÃ§ adÄ±mda hesaplanacaÄŸÄ± anlamÄ±na gelir:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>&alpha;(z<sub>1</sub>)+b<sub>2</sub>
* f = &sigma;(z<sub>2</sub>)

Burada, &alpha; bir **doÄŸrusal olmayan etkinleÅŸtirme iÅŸlevidir**, &sigma; bir softmaks iÅŸlevidir ve &theta;=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*> parametreleridir.

Gradyan iniÅŸi algoritmasÄ± aynÄ± kalacaktÄ±r, ancak gradyanlarÄ± hesaplamak daha zor olacaktÄ±r. Zincir tÃ¼rev alma kuralÄ± gÃ¶z Ã¶nÃ¼ne alÄ±ndÄ±ÄŸÄ±nda, tÃ¼revleri ÅŸu ÅŸekilde hesaplayabiliriz:

* &part;&lagran;/&part;w<sub>2</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;w<sub>2</sub>)
* &part;&lagran;/&part;w<sub>1</sub> = (&part;&lagran;/&part;&sigma;)(&part;&sigma;/&part;z<sub>2</sub>)(&part;z<sub>2</sub>/&part;&alpha;)(&part;&alpha;/&part;z<sub>1</sub>)(&part;z<sub>1</sub>/&part;w<sub>1</sub>)

> âœ… Parametrelere gÃ¶re kayÄ±p fonksiyonunun tÃ¼revlerini hesaplamak iÃ§in zincir tÃ¼rev alma kuralÄ± kullanÄ±lÄ±r.

TÃ¼m bu ifadelerin en soldaki kÄ±smÄ±nÄ±n aynÄ± olduÄŸuna ve dolayÄ±sÄ±yla kayÄ±p fonksiyonundan baÅŸlayarak ve hesaplama Ã§izgesi boyunca "geriye" doÄŸru giden tÃ¼revleri etkin bir ÅŸekilde hesaplayabileceÄŸimize dikkat edin. Bu nedenle Ã§ok katmanlÄ± bir algÄ±layÄ±cÄ±yÄ± eÄŸitme yÃ¶ntemine **geri yayma** veya 'geriyay' denir.

<img alt="compute graph" src="../images/ComputeGraphGrad.png"/>

> TODO: imge alÄ±ntÄ±sÄ±

> âœ… Geriyay'Ä± not defteri Ã¶rneÄŸimizde Ã§ok daha ayrÄ±ntÄ±lÄ± olarak ele alacaÄŸÄ±z.

## VargÄ±lar

Bu dersimizde kendi sinir aÄŸÄ± kÃ¼tÃ¼phanemizi oluÅŸturduk ve onu basit bir iki boyutlu sÄ±nÄ±flandÄ±rma gÃ¶revi iÃ§in kullandÄ±k.

## ğŸš€ Kendini SÄ±nama

Ekteki not defterinde, Ã§ok katmanlÄ± algÄ±layÄ±cÄ±larÄ± oluÅŸturmak ve eÄŸitmek iÃ§in kendi Ã§erÃ§evenizi uygulayacaksÄ±nÄ±z. Modern sinir aÄŸlarÄ±nÄ±n nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± ayrÄ±ntÄ±lÄ± olarak gÃ¶rebileceksiniz.

[OwnFramework (KendiCerveceniz)](OwnFramework.tr.ipynb) not defterine gidin ve Ã¼zerinde Ã§alÄ±ÅŸÄ±n.

## [Ders sonrasÄ± sÄ±navÄ±](https://red-field-0a6ddfd03.1.azurestaticapps.net/quiz/204)

## GÃ¶zden GeÃ§irme ve Bireysel Ã‡alÄ±ÅŸma

Geri yayma, YZ ve makine Ã¶ÄŸrenmesinde kullanÄ±lan yaygÄ±n bir algoritmadÄ±r ve [daha ayrÄ±ntÄ±lÄ± olarak](https://wikipedia.org/wiki/Backpropagation) incelemeye deÄŸerdir.

## [Ã–dev](../lab/translations/README.tr.md)

Bu laboratuvarda, MNIST el yazÄ±sÄ± rakam sÄ±nÄ±flandÄ±rmasÄ±nÄ± Ã§Ã¶zmek iÃ§in bu derste oluÅŸturduÄŸunuz Ã§erÃ§eveyi kullanmanÄ±z istenmektedir.

* [Talimatlar](../lab/translations/README.tr.md)
* [Not Defteri](../lab/translations/MyFW_MNIST.tr.ipynb)
