{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Çok Katmanlı Algılayıcılar\n",
    "## Kendi Sinirsel Çerçevemizi Oluşturmak\n",
    "\n",
    "> Bu not defteri, [Yeni Başlayanlar için YZ Müfredatı](http://github.com/microsoft/ai-for-beginners)'nın bir parçasıdır. Eksiksiz öğrenme materyalleri kümesi için kaynak deposunu ziyaret edin.\n",
    "\n",
    "Bu defterde, çok-katmanlı algılayıcılarla çok-sınıflı sınıflandırma görevlerini ve bağlanımı çözebilen kendi sinirsel çerçevemizi yavaş yavaş inşa edeceğiz.\n",
    "\n",
    "İlk olarak, bazı gerekli kütüphaneleri içe aktaralım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import gridspec\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "# tekrarlanabilirlik için tohumu (seed) seçin - rastgele değişğmlerin etkilerini keşfetmek için değiştirin\n",
    "np.random.seed(0)\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Numune Veri Kümesi\n",
    "\n",
    "Daha önce olduğu gibi, iki parametreli basit bir örnek veri kümesi ile başlayacağız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "X, Y = make_classification(n_samples = n, n_features=2,\n",
    "                           n_redundant=0, n_informative=2, flip_y=0.2)\n",
    "X = X.astype(np.float32)\n",
    "Y = Y.astype(np.int32)\n",
    "\n",
    "# Eğitim ve test veri kümesine bölün\n",
    "train_x, test_x = np.split(X, [n*8//10])\n",
    "train_labels, test_labels = np.split(Y, [n*8//10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_dataset(suptitle, features, labels):\n",
    "    # görseli hazırla\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    #pylab.subplots_adjust(bottom=0.2, wspace=0.4)\n",
    "    fig.suptitle(suptitle, fontsize = 16)\n",
    "    ax.set_xlabel('$x_i[0]$ -- (öznitelik 1)')\n",
    "    ax.set_ylabel('$x_i[1]$ -- (öznitelik 2)')\n",
    "\n",
    "    colors = ['r' if l else 'b' for l in labels]\n",
    "    ax.scatter(features[:, 0], features[:, 1], marker='o', c=colors, s=100, alpha = 0.5)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_dataset('Eğitim verilerinin dağıtık grafiği', train_x, train_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x[:5])\n",
    "print(train_labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Makine Öğrenmesi Problemi\n",
    "\n",
    "Diyelim ki $\\langle X,Y\\rangle$ veri setimiz var, burada $X$ bir dizi öznitelik ve $Y$ karşılık gelen etiketlerdir. Bağlanım problemi için $y_i\\in\\mathbb{R}$ ve sınıflandırma için $y_i\\in\\{0,\\dots,n\\}$ sınıf numarası ile temsil edilir.\n",
    "\n",
    "Herhangi bir makine öğrenmesi modeli, $f_\\theta(x)$ işleviyle temsil edilebilir, burada $\\theta$ bir **parametre** kümesidir. Amacımız, modelimizin veri kümesine en iyi şekilde uyduğu $\\theta$ parametrelerini bulmaktır. Kriterler **kayıp işlevi** $\\mathcal{L}$ tarafından tanımlanır ve en uygun değeri bulmamız gerekir\n",
    "\n",
    "$$\n",
    "\\theta = \\mathrm{argmin}_\\theta \\mathcal{L}(f_\\theta(X),Y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Kayıp işlevi çözülen probleme bağlıdır.\n",
    "\n",
    "### Bağlanım için kayıp işlevleri\n",
    "\n",
    "Bağlanım için genellikle **mutlak hata** $\\mathcal{L}_{mutlak}(\\theta) = \\sum_{i=1}^n |y_i - f_{\\theta}(x_i)|$, veya **ortalama kare hatası** $\\mathcal{L}_{sq}(\\theta) = \\sum_{i=1}^n (y_i - f_{\\theta}(x_i))^2$ kullanırız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# çeşitli kayıp fonksiyonlarını çizmek için yardımcı fonksiyon\n",
    "def plot_loss_functions(suptitle, functions, ylabels, xlabel):\n",
    "    fig, ax = plt.subplots(1,len(functions), figsize=(9, 3))\n",
    "    plt.subplots_adjust(bottom=0.2, wspace=0.4)\n",
    "    fig.suptitle(suptitle)\n",
    "    for i, fun in enumerate(functions):\n",
    "        ax[i].set_xlabel(xlabel)\n",
    "        if len(ylabels) > i:\n",
    "            ax[i].set_ylabel(ylabels[i])\n",
    "        ax[i].plot(x, fun)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-2, 2, 101)\n",
    "plot_loss_functions(\n",
    "    suptitle = 'Bağlanım için ortak kayıp fonksiyonları',\n",
    "    functions = [np.abs(x), np.power(x, 2)],\n",
    "    ylabels   = ['$\\mathcal{L}_{abs}}$ (mutlak kayıp)',\n",
    "                 '$\\mathcal{L}_{sq}$ (kare kayıp)'],\n",
    "    xlabel    = '$y - f(x_i)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sınıflandırma için kayıp fonksiyonları\n",
    "\n",
    "Bir an için ikili sınıflandırmayı ele alalım. Bu durumda 0 ve 1 olarak numaralandırılmış iki sınıfımız var. $f_\\theta(x_i)\\in [0,1]$ ağının çıktısı esasen sınıf 1'in seçilme olasılığını tanımlar.\n",
    "\n",
    "**0-1 kaybı**\n",
    "\n",
    "0-1 kaybı, modelin doğruluğunu hesaplamakla aynıdır - doğru sınıflandırmaların sayısını hesaplarız:\n",
    "\n",
    "$$\\mathcal{L}_{0-1} = \\sum_{i=1}^n l_i \\quad  l_i = \\begin{cases}\n",
    "         0 & (f(x_i)<0.5 \\land y_i=0) \\lor (f(x_i)<0.5 \\land y_i=1) \\\\\n",
    "         1 & \\mathrm{ diğer~türlü}\n",
    "       \\end{cases} \\\\\n",
    "$$\n",
    "\n",
    "Ancak doğruluğun kendisi, doğru sınıflandırmadan ne kadar uzak olduğumuzu göstermez. Doğru sınıfı birazcık kaçırmış olabiliriz ve bu, bir bakımdan önemli ölçüde kaçırmaktan \"daha iyi\" (bir anlamda ağırlıkları çok daha az düzeltmemiz gerekiyor). Bu nedenle, genellikle bunu dikkate alan lojistik kayıp kullanılır.\n",
    "\n",
    "**Logistik Kayıp**\n",
    "\n",
    "$$\\mathcal{L}_{log} = \\sum_{i=1}^n -y\\log(f_{\\theta}(x_i)) - (1-y)\\log(1-f_\\theta(x_i))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,1,100)\n",
    "def zero_one(d):\n",
    "    if d < 0.5:\n",
    "        return 0\n",
    "    return 1\n",
    "zero_one_v = np.vectorize(zero_one)\n",
    "\n",
    "def logistic_loss(fx):\n",
    "    # assumes y == 1\n",
    "    return -np.log(fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_loss_functions(suptitle = 'Sınıflandırma için ortak kayıp fonksiyonları (sınıf=1)',\n",
    "                   functions = [zero_one_v(x), logistic_loss(x)],\n",
    "                   ylabels    = ['$\\mathcal{L}_{0-1}}$ (0-1 kaybı)',\n",
    "                                 '$\\mathcal{L}_{log}$ (logistic kayıp)'],\n",
    "                   xlabel     = '$p$')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lojistik kaybı anlamak için, beklenen çıktının iki durumunu göz önünde bulundurun:\n",
    "* Çıktının 1 ($y=1$) olmasını beklersek, kayıp $-log f_\\theta(x_i)$ olur. Kayıp 0'dır, ağ 1 olasılıkla 1'i tahmin eder ve 1 olasılığı küçüldüğünde büyür.\n",
    "* Çıktının 0 ($y=0$) olmasını beklersek, kayıp $-log(1-f_\\theta(x_i))$ olur. Burada, $1-f_\\theta(x_i)$, ağ tarafından tahmin edilen 0 olasılığıdır ve logaritmik kaybın anlamı, önceki durumda açıklananla aynıdır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sinir Ağı Mimarisi\n",
    "\n",
    "İkili sınıflandırma problemi için bir veri kümesi oluşturduk. Ancak, bunu en baştan çok sınıflı sınıflandırma olarak düşünelim, böylece kodumuzu kolayca çok sınıflı sınıflandırmaya çevirebiliriz. Bu durumda, tek katmanlı algılayıcımız aşağıdaki mimariye sahip olacaktır:\n",
    "\n",
    "<img src=\"../images/NeuroArch.png\" width=\"50%\"/>\n",
    "\n",
    "Ağın iki çıktısı iki sınıfa karşılık gelir ve iki çıktı arasında en yüksek değere sahip sınıf doğru çözüme karşılık gelir.\n",
    "\n",
    "Model şöyle tanımlanır:\n",
    "$$\n",
    "f_\\theta(x) = W\\times x + b\n",
    "$$\n",
    "burada $$\\theta = \\langle W,b\\rangle$$ parametrelerdir.\n",
    "\n",
    "Bu doğrusal katmanı, hesaplamayı gerçekleştiren `forward` (ileri) işlevi olan bir Python sınıfı olarak tanımlayacağız. $x$ girdi değerini alır ve katmanın çıktısını üretir. `W` ve `b` parametreleri katman sınıfı içinde depolanır ve yaratıldığında sırasıyla rastgele değerler ve sıfırlarla ilklenir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self,nin,nout):\n",
    "        self.W = np.random.normal(0, 1.0/np.sqrt(nin), (nout, nin))\n",
    "        self.b = np.zeros((1,nout))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return np.dot(x, self.W.T) + self.b\n",
    "    \n",
    "net = Linear(2,2)\n",
    "net.forward(train_x[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Çoğu durumda, tek bir girdi değeri üzerinde değil, girdi değerlerinin vektörü üzerinde çalışmak daha verimlidir. Numpy işlemlerini kullandığımız için, ağımıza bir girdi değerleri vektörü geçirebiliriz ve bu bize çıktı değerlerinin vektörünü verecektir.\n",
    "\n",
    "## Softmaks: Çıktıları Olasılıklara Dönüştürme\n",
    "\n",
    "Gördüğünüz gibi, çıktılarımız olasılık değil - herhangi bir değer alabilirler. Bunları olasılıklara dönüştürmek için tüm sınıflardaki değerleri normalleştirmemiz gerekir. Bu, **softmaks** işlevi kullanılarak yapılır: $$\\sigma(\\mathbf{z}_c) = \\frac{e^{z_c}}{\\sum_{j} e^{z_j}}, \\quad\\mathrm {öyleki}\\quad c\\in 1 .. |C|$$\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/images/NeuroArch-softmax.PNG\" width=\"50%\">\n",
    "\n",
    "> Ağın çıktısı $\\sigma(\\mathbf{z})$, $C$ sınıflar kümesinde olasılık dağılımı olarak yorumlanabilir: $q = \\sigma(\\mathbf{z}_c) = \\hat{p} (c | x)$\n",
    "\n",
    "`Softmax` (Softmaks) katmanını, `forward` (ileri) işlevli bir sınıfla aynı şekilde tanımlayacağız:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self,z):\n",
    "        zmax = z.max(axis=1,keepdims=True)\n",
    "        expz = np.exp(z-zmax)\n",
    "        Z = expz.sum(axis=1,keepdims=True)\n",
    "        return expz / Z\n",
    "\n",
    "softmax = Softmax()\n",
    "softmax.forward(net.forward(train_x[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Artık çıktılar olarak olasılıklar aldığımızı görebilirsiniz, yani her çıktı vektörünün toplamı tam olarak 1'dir.\n",
    "\n",
    "2'den fazla sınıfımız olması durumunda, softmaks bunların hepsinde olasılıkları normalleştirir. İşte MNIST rakam sınıflandırması yapan bir ağ mimarisi diyagramı:\n",
    "\n",
    "![MNIST Classifier](../images/Cross-Entropy-Loss.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Çapraz Entropi Kaybı\n",
    "\n",
    "Sınıflandırmadaki bir kayıp fonksiyonu, tipik olarak, **çapraz entropi kaybı** olarak genelleştirilebilen bir lojistik fonksiyondur. Çapraz entropi kaybı, iki rastgele olasılık dağılımı arasındaki benzerliği hesaplayabilen bir fonksiyondur. Bununla ilgili daha ayrıntılı tartışmayı [Wikipedia](https://en.wikipedia.org/wiki/Cross_entropy)'da bulabilirsiniz.\n",
    "\n",
    "Bizim durumumuzda, ilk dağılım ağımızın olasılıksal çıktısıdır ve ikincisi **bire bir** dağılım olarak adlandırılır, bu da belirli bir $c$ sınıfının 1'e karşılık gelen olasılığa sahip olduğunu belirtir (geri kalan her şey 0'dır). Böyle bir durumda çapraz entropi kaybı $-\\log p_c$ olarak hesaplanabilir, burada $c$ beklenen sınıftır ve $p_c$, sinir ağımız tarafından verilen bu sınıfa karşılık gelen olasılıktır.\n",
    "\n",
    "> Beklenen sınıf için ağ 1 olasılığını döndürürse, çapraz entropi kaybı 0 olur. Gerçek sınıfın olasılığı 0'a ne kadar yakınsa, çapraz entropi kaybı o kadar yüksek olur (ve sonsuza kadar gidebilir!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_cross_ent():\n",
    "    p = np.linspace(0.01, 0.99, 101) # tahmin edilen olasılık p(y|x)\n",
    "    cross_ent_v = np.vectorize(cross_ent)\n",
    "    f3, ax = plt.subplots(1,1, figsize=(8, 3))\n",
    "    l1, = plt.plot(p, cross_ent_v(p, 1), 'r--')\n",
    "    l2, = plt.plot(p, cross_ent_v(p, 0), 'r-')\n",
    "    plt.legend([l1, l2], ['$y = 1$', '$y = 0$'], loc = 'upper center', ncol = 2)\n",
    "    plt.xlabel('$\\hat{p}(y|x)$', size=18)\n",
    "    plt.ylabel('$\\mathcal{L}_{CE}$', size=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def cross_ent(prediction, ground_truth):\n",
    "    t = 1 if ground_truth > 0.5 else 0\n",
    "    return -t * np.log(prediction) - (1 - t) * np.log(1 - prediction)\n",
    "plot_cross_ent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Çapraz entropi kaybı yine ayrı bir katman olarak tanımlanacak, ancak `forward` (ileri) işlevi iki girdi değerine sahip olacaktır: Ağın önceki katmanlarının çıktısı `p` ve beklenen sınıf `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def forward(self,p,y):\n",
    "        self.p = p\n",
    "        self.y = y\n",
    "        p_of_y = p[np.arange(len(y)), y]\n",
    "        log_prob = np.log(p_of_y)\n",
    "        return -log_prob.mean() # tüm girdi örnekleri üzerinden ortalama\n",
    "\n",
    "cross_ent_loss = CrossEntropyLoss()\n",
    "p = softmax.forward(net.forward(train_x[0:10]))\n",
    "cross_ent_loss.forward(p,train_labels[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> **ÖNEMLİ**: Kayıp işlevi, ağımızın ne kadar iyi (veya kötü) performans gösterdiğini gösteren bir sayı döndürür. Veri kümesinin tamamı veya veri kümesinin bir kısmı (minigrup) için bize bir sayı döndürmelidir. Bu nedenle, girdi vektörünün her bir bileşeni için çapraz entropi kaybını hesapladıktan sonra, tüm bileşenlerin ortalamasını almamız (veya toplamamız) gerekir - bu, `.mean()` çağrısıyla yapılır.\n",
    "\n",
    "## Hesaplamalı Çizge\n",
    "\n",
    "<img src=\"../images/ComputeGraph.png\" width=\"600px\"/>\n",
    "\n",
    "Bu ana kadar ağın farklı katmanları için farklı sınıflar tanımladık. Bu katmanların bileşimi **hesaplamalı çizge** olarak gösterilebilir. Şimdi, belirli bir eğitim veri kümesi (veya bunun bir parçası) için kaybı aşağıdaki şekilde hesaplayabiliriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = net.forward(train_x[0:10])\n",
    "p = softmax.forward(z)\n",
    "loss = cross_ent_loss.forward(p,train_labels[0:10])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kaybı En Aza İndirme Problemi ve Ağ Eğitimi\n",
    "\n",
    "Ağı $f_\\theta$ olarak tanımladıktan ve $\\mathcal{L}(Y,f_\\theta(X))$ kayıp fonksiyonu verildikten sonra, sabit eğitim veri kümemiz altında $\\mathcal{L}$'yi $\\theta$'nın bir fonksiyonu olarak düşünebiliriz.  : $\\mathcal{L}(\\theta) = \\mathcal{L}(Y,f_\\theta(X))$\n",
    "\n",
    "Bu durumda, ağ eğitimi $\\theta$ bağımsız değişkeni altında $\\mathcal{L}$ en aza indirme (minimizasyon) problemi olacaktır:\n",
    "$$\n",
    "\\theta = \\mathrm{argmin}_{\\theta} \\mathcal{L}(Y,f_\\theta(X))\n",
    "$$\n",
    "\n",
    "**Gradyan inişi** adı verilen iyi bilinen bir işlev eniyileme (optimizasyon) yöntemi vardır. Buradaki fikir, parametrelere göre kayıp fonksiyonunun bir türevini (çok boyutlu durumda **gradyan** olarak adlandırılır) hesaplayabilmemiz ve parametreleri, hatanın azalacağı şekilde değiştirebilmemizdir.\n",
    "\n",
    "Gradyan inişi şu şekilde çalışır:\n",
    "  * Parametreleri, $w^{(0)}$ ve $b^{(0)}$, bazı rasgele değerlerle ilkletin.\n",
    "  * Aşağıdaki adımı birçok kez tekrarlayın:\n",
    "\n",
    " $$\\begin{align}\n",
    " W^{(i+1)}&=W^{(i)}-\\eta\\frac{\\partial\\mathcal{L}}{\\partial W}\\\\\n",
    " b^{(i+1)}&=b^{(i)}-\\eta\\frac{\\partial\\mathcal{L}}{\\partial b}\n",
    " \\end{align}\n",
    " $$\n",
    "\n",
    "Eğitim esnasında eniyileme adımlarının tüm veri kümesi dikkate alınarak hesaplanması gerekir (kaybın tüm eğitim örnekleri üzerinden bir toplam/ortalama olarak hesaplandığını unutmayın). Bununla birlikte, gerçek hayatta **minigruplar** olarak adlandırılan veri kümesinin küçük kısımlarını alır ve bir veri alt kümesine dayalı olarak gradyanları hesaplarız. Alt küme her seferinde rastgele alındığından, bu yönteme **rasgele gradyan inişi** (SGD - RGİ) denir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Geriye Yayma\n",
    "\n",
    "<img src=\"../images/ComputeGraph.png\" width=\"300px\" align=\"left\"/>\n",
    "\n",
    "$$\\def\\L{\\mathcal{L}}\\def\\zz#1#2{\\frac{\\partial#1}{\\partial#2}}\n",
    "\\begin{align}\n",
    "\\zz{\\L}{W} =& \\zz{\\L}{p}\\zz{p}{z}\\zz{z}{W}\\cr\n",
    "\\zz{\\L}{b} =& \\zz{\\L}{p}\\zz{p}{z}\\zz{z}{b}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\partial\\mathcal{L}/\\partial W$'ı hesaplamak için, yukarıdaki formüllerde görebileceğiniz gibi, bileşik bir fonksiyonun türevlerini hesaplamak için **zincirleme kuralını** kullanabiliriz. Aşağıdaki fikre karşılık gelir:\n",
    "\n",
    "* Verilen girdinin altında $\\Delta\\mathcal{L}$ kaybını elde ettiğimizi varsayalım.\n",
    "* Bunu en aza indirmek için, softmaks çıktısı $p$'yı $\\Delta p = (\\partial\\mathcal{L}/\\partial p)\\Delta\\mathcal{L}$ değerine göre ayarlamamız gerekir.\n",
    "* Bu, $z$ düğümünde $\\Delta z = (\\partial\\mathcal{p}/\\partial z)\\Delta p$ tarafından yapılan değişikliklere karşılık gelir.\n",
    "* Bu hatayı en aza indirmek için parametreleri buna göre ayarlamamız gerekiyor: $\\Delta W = (\\partial\\mathcal{z}/\\partial W)\\Delta z$ (ve $b$ için aynı)\n",
    "\n",
    "<img src=\"../images/ComputeGraphGrad.PNG\" width=\"400px\" align=\"right\"/>\n",
    "\n",
    "Bu işlem, kayıp hatasını ağın çıktısından parametrelerine geri dağıtmaya başlar. Bu nedenle işlem **geri yayma** olarak adlandırılır.\n",
    "\n",
    "Ağ eğitiminin bir geçişi iki bölümden oluşur:\n",
    "* Belirli bir girdi minigrubu için kayıp fonksiyonunun değerini hesapladığımızda **ileriye geçiş** \n",
    "* Bu hatayı hesaplamalı çizge aracılığıyla model parametrelerine geri dağıtarak en aza indirmeye çalıştığımızda **geriye geçiş**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geri Yaymanın Uygulanması\n",
    "\n",
    "* Geriye geçiş sırasında türevi hesaplayacak ve hatayı yayacak olan düğümlerimizin her birine `backward` (geri) fonksiyonunu ekleyelim.\n",
    "* Ayrıca yukarıda açıklanan prosedüre göre parametre güncellemelerini de uygulamamız gerekiyor.\n",
    "\n",
    "Her katman için türevleri elle hesaplamamız gerekir, örneğin doğrusal katman $z = x\\times W+b$ için:\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial z}{\\partial W} &= x \\\\\n",
    "\\frac{\\partial z}{\\partial b} &= 1 \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Katmanın çıktısındaki $\\Delta z$ hatasını telafi etmemiz gerekirse, ağırlıkları buna göre güncellememiz gerekir:\n",
    "$$\\begin{align}\n",
    "\\Delta x &= \\Delta z \\times W \\\\\n",
    "\\Delta W &= \\frac{\\partial z}{\\partial W} \\Delta z = \\Delta z \\times x \\\\\n",
    "\\Delta b &= \\frac{\\partial z}{\\partial b} \\Delta z = \\Delta z \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "**ÖNEMLİ:** Hesaplamalar her eğitim örneği için bağımsız olarak değil, bir bütün **minigrup** için yapılır. Gerekli parametre güncellemeleri $\\Delta W$ ve $\\Delta b$ tüm minigrupta hesaplanır ve ilgili vektörlerin $x\\in\\mathbb{R}^{\\mathrm{minigrup}\\, \\times\\, \\mathrm{sınıfsayısı}}$ boyutları vardır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self,nin,nout):\n",
    "        self.W = np.random.normal(0, 1.0/np.sqrt(nin), (nout, nin))\n",
    "        self.b = np.zeros((1,nout))\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x=x\n",
    "        return np.dot(x, self.W.T) + self.b\n",
    "    \n",
    "    def backward(self, dz):\n",
    "        dx = np.dot(dz, self.W)\n",
    "        dW = np.dot(dz.T, self.x)\n",
    "        db = dz.sum(axis=0)\n",
    "        self.dW = dW\n",
    "        self.db = db\n",
    "        return dx\n",
    "    \n",
    "    def update(self,lr):\n",
    "        self.W -= lr*self.dW\n",
    "        self.b -= lr*self.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aynı şekilde, katmanlarımızın geri kalanı için `backward` (geri) işlevi tanımlayabiliriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self,z):\n",
    "        self.z = z\n",
    "        zmax = z.max(axis=1,keepdims=True)\n",
    "        expz = np.exp(z-zmax)\n",
    "        Z = expz.sum(axis=1,keepdims=True)\n",
    "        return expz / Z\n",
    "    def backward(self,dp):\n",
    "        p = self.forward(self.z)\n",
    "        pdp = p * dp\n",
    "        return pdp - p * pdp.sum(axis=1, keepdims=True)\n",
    "    \n",
    "class CrossEntropyLoss:\n",
    "    def forward(self,p,y):\n",
    "        self.p = p\n",
    "        self.y = y\n",
    "        p_of_y = p[np.arange(len(y)), y]\n",
    "        log_prob = np.log(p_of_y)\n",
    "        return -log_prob.mean()\n",
    "    def backward(self,loss):\n",
    "        dlog_softmax = np.zeros_like(self.p)\n",
    "        dlog_softmax[np.arange(len(self.y)), self.y] -= 1.0/len(self.y)\n",
    "        return dlog_softmax / self.p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now we are ready to write the **training loop**, which will go through our dataset, and perform the optimization minibatch by minibatch.One complete pass through the dataset is often called **an epoch**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = Linear(2,2)\n",
    "softmax = Softmax()\n",
    "cross_ent_loss = CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "pred = np.argmax(lin.forward(train_x),axis=1)\n",
    "acc = (pred==train_labels).mean()\n",
    "print(\"Initial accuracy: \",acc)\n",
    "\n",
    "batch_size=4\n",
    "for i in range(0,len(train_x),batch_size):\n",
    "    xb = train_x[i:i+batch_size]\n",
    "    yb = train_labels[i:i+batch_size]\n",
    "    \n",
    "    # forward pass\n",
    "    z = lin.forward(xb)\n",
    "    p = softmax.forward(z)\n",
    "    loss = cross_ent_loss.forward(p,yb)\n",
    "    \n",
    "    # backward pass\n",
    "    dp = cross_ent_loss.backward(loss)\n",
    "    dz = softmax.backward(dp)\n",
    "    dx = lin.backward(dz)\n",
    "    lin.update(learning_rate)\n",
    "    \n",
    "pred = np.argmax(lin.forward(train_x),axis=1)\n",
    "acc = (pred==train_labels).mean()\n",
    "print(\"Final accuracy: \",acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice to see how we can increase accuracy of the model from about 50% to around 80% in one epoch.\n",
    "\n",
    "## Network Class\n",
    "\n",
    "Since in many cases neural network is just a composition of layers, we can build a class that will allow us to stack layers together and make forward and backward passes through them without explicitly programming that logic. We will store the list of layers inside the `Net` class, and use `add()` function to add new layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class Net:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add(self,l):\n",
    "        self.layers.append(l)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for l in self.layers:\n",
    "            x = l.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self,z):\n",
    "        for l in self.layers[::-1]:\n",
    "            z = l.backward(z)\n",
    "        return z\n",
    "    \n",
    "    def update(self,lr):\n",
    "        for l in self.layers:\n",
    "            if 'update' in l.__dir__():\n",
    "                l.update(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this `Net` class our model definition and training becomes more neat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.add(Linear(2,2))\n",
    "net.add(Softmax())\n",
    "loss = CrossEntropyLoss()\n",
    "\n",
    "def get_loss_acc(x,y,loss=CrossEntropyLoss()):\n",
    "    p = net.forward(x)\n",
    "    l = loss.forward(p,y)\n",
    "    pred = np.argmax(p,axis=1)\n",
    "    acc = (pred==y).mean()\n",
    "    return l,acc\n",
    "\n",
    "print(\"Initial loss={}, accuracy={}: \".format(*get_loss_acc(train_x,train_labels)))\n",
    "\n",
    "def train_epoch(net, train_x, train_labels, loss=CrossEntropyLoss(), batch_size=4, lr=0.1):\n",
    "    for i in range(0,len(train_x),batch_size):\n",
    "        xb = train_x[i:i+batch_size]\n",
    "        yb = train_labels[i:i+batch_size]\n",
    "\n",
    "        p = net.forward(xb)\n",
    "        l = loss.forward(p,yb)\n",
    "        dp = loss.backward(l)\n",
    "        dx = net.backward(dp)\n",
    "        net.update(lr)\n",
    " \n",
    "train_epoch(net,train_x,train_labels)\n",
    "        \n",
    "print(\"Final loss={}, accuracy={}: \".format(*get_loss_acc(train_x,train_labels)))\n",
    "print(\"Test loss={}, accuracy={}: \".format(*get_loss_acc(test_x,test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Training Process\n",
    "\n",
    "It would be nice to see visually how the network is being trained! We will define a `train_and_plot` function for that. To visualize the state of the network we will use level map, i.e. we will represent different values of the network output using different colors.\n",
    "\n",
    "> Do not worry if you do not understand some of the plotting code below - it is more important to understand the underlying neural network concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_plot(n_epoch, net, loss=CrossEntropyLoss(), batch_size=4, lr=0.1):\n",
    "    fig, ax = plt.subplots(2, 1)\n",
    "    ax[0].set_xlim(0, n_epoch + 1)\n",
    "    ax[0].set_ylim(0,1)\n",
    "\n",
    "    train_acc = np.empty((n_epoch, 3))\n",
    "    train_acc[:] = np.NAN\n",
    "    valid_acc = np.empty((n_epoch, 3))\n",
    "    valid_acc[:] = np.NAN\n",
    "\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "\n",
    "        train_epoch(net,train_x,train_labels,loss,batch_size,lr)\n",
    "        tloss, taccuracy = get_loss_acc(train_x,train_labels,loss)\n",
    "        train_acc[epoch-1, :] = [epoch, tloss, taccuracy]\n",
    "        vloss, vaccuracy = get_loss_acc(test_x,test_labels,loss)\n",
    "        valid_acc[epoch-1, :] = [epoch, vloss, vaccuracy]\n",
    "        \n",
    "        ax[0].set_ylim(0, max(max(train_acc[:, 2]), max(valid_acc[:, 2])) * 1.1)\n",
    "\n",
    "        plot_training_progress(train_acc[:, 0], (train_acc[:, 2],\n",
    "                                                 valid_acc[:, 2]), fig, ax[0])\n",
    "        plot_decision_boundary(net, fig, ax[1])\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "\n",
    "    return train_acc, valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "def plot_decision_boundary(net, fig, ax):\n",
    "    draw_colorbar = True\n",
    "    # remove previous plot\n",
    "    while ax.collections:\n",
    "        ax.collections.pop()\n",
    "        draw_colorbar = False\n",
    "\n",
    "    # generate countour grid\n",
    "    x_min, x_max = train_x[:, 0].min() - 1, train_x[:, 0].max() + 1\n",
    "    y_min, y_max = train_x[:, 1].min() - 1, train_x[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                         np.arange(y_min, y_max, 0.1))\n",
    "    grid_points = np.c_[xx.ravel().astype('float32'), yy.ravel().astype('float32')]\n",
    "    n_classes = max(train_labels)+1\n",
    "    while train_x.shape[1] > grid_points.shape[1]:\n",
    "        # pad dimensions (plot only the first two)\n",
    "        grid_points = np.c_[grid_points,\n",
    "                            np.empty(len(xx.ravel())).astype('float32')]\n",
    "        grid_points[:, -1].fill(train_x[:, grid_points.shape[1]-1].mean())\n",
    "\n",
    "    # evaluate predictions\n",
    "    prediction = np.array(net.forward(grid_points))\n",
    "    # for two classes: prediction difference\n",
    "    if (n_classes == 2):\n",
    "        Z = np.array([0.5+(p[0]-p[1])/2.0 for p in prediction]).reshape(xx.shape)\n",
    "    else:\n",
    "        Z = np.array([p.argsort()[-1]/float(n_classes-1) for p in prediction]).reshape(xx.shape)\n",
    "    \n",
    "    # draw contour\n",
    "    levels = np.linspace(0, 1, 40)\n",
    "    cs = ax.contourf(xx, yy, Z, alpha=0.4, levels = levels)\n",
    "    if draw_colorbar:\n",
    "        fig.colorbar(cs, ax=ax, ticks = [0, 0.5, 1])\n",
    "    c_map = [cm.jet(x) for x in np.linspace(0.0, 1.0, n_classes) ]\n",
    "    colors = [c_map[l] for l in train_labels]\n",
    "    ax.scatter(train_x[:, 0], train_x[:, 1], marker='o', c=colors, s=60, alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_training_progress(x, y_data, fig, ax):\n",
    "    styles = ['k--', 'g-']\n",
    "    # remove previous plot\n",
    "    while ax.lines:\n",
    "        ax.lines.pop()\n",
    "    # draw updated lines\n",
    "    for i in range(len(y_data)):\n",
    "        ax.plot(x, y_data[i], styles[i])\n",
    "    ax.legend(ax.lines, ['training accuracy', 'validation accuracy'],\n",
    "              loc='upper center', ncol = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg \n",
    "net = Net()\n",
    "net.add(Linear(2,2))\n",
    "net.add(Softmax())\n",
    "\n",
    "res = train_and_plot(30,net,lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After running the cell above you should be able to see interactively how the boundary between classes change during training. Note that we have chosen very small learning rate so that we can see how the process happens.\n",
    "\n",
    "## Multi-Layered Models\n",
    "\n",
    "The network above has been constructed from several layers, but we still had only one `Linear` layer, which does the actual classification. What happens if we decide to add several such layers?\n",
    "\n",
    "Surprisingly, our code will work! Very important thing to note, however, is that in between linear layers we need to have a non-linear **activation function**, such as `tanh`. Without such non-linearity, several linear layers would have the same expressive power as just one layers - because composition of linear functions is also linear!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(self,x):\n",
    "        y = np.tanh(x)\n",
    "        self.y = y\n",
    "        return y\n",
    "    def backward(self,dy):\n",
    "        return (1.0-self.y**2)*dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Adding several layers make sense, because unlike one-layer network, multi-layered model will be able to accuratley classify sets that are not linearly separable. I.e., a model with several layers will be **reacher**.\n",
    "\n",
    "> It can be demonstrated that with sufficient number of neurons a two-layered model is capable to classifying any convex set of data points, and three-layered network can classify virtually any set.\n",
    "\n",
    "Mathematically, multi-layered perceptron would be represented by a more complex function $f_\\theta$ that can be computed in several steps:\n",
    "* $z_1 = W_1\\times x+b_1$\n",
    "* $z_2 = W_2\\times\\alpha(z_1)+b_2$\n",
    "* $f = \\sigma(z_2)$\n",
    "\n",
    "Here, $\\alpha$ is a **non-linear activation function**, $\\sigma$ is a softmax function, and $\\theta=\\langle W_1,b_1,W_2,b_2\\rangle$ are parameters.\n",
    "\n",
    "The gradient descent algorithm would remain the same, but it would be more difficult to calculate gradients. Given the\n",
    " chain differentiation rule, we can calculate derivatives as:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial W_2} &= \\color{red}{\\frac{\\partial\\mathcal{L}}{\\partial\\sigma}\\frac{\\partial\\sigma}{\\partial z_2}}\\color{black}{\\frac{\\partial z_2}{\\partial W_2}} \\\\\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial W_1} &= \\color{red}{\\frac{\\partial\\mathcal{L}}{\\partial\\sigma}\\frac{\\partial\\sigma}{\\partial z_2}}\\color{black}{\\frac{\\partial z_2}{\\partial\\alpha}\\frac{\\partial\\alpha}{\\partial z_1}\\frac{\\partial z_1}{\\partial W_1}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that the beginning of all those expressions is still the same, and thus we can continue back propagation beyond one linear layers to adjust further weights up the computational graph.\n",
    "\n",
    "Let's now experiment with two-layered network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.add(Linear(2,10))\n",
    "net.add(Tanh())\n",
    "net.add(Linear(10,2))\n",
    "net.add(Softmax())\n",
    "loss = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "res = train_and_plot(30,net,lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Not Always Use Multi-Layered Model?\n",
    "\n",
    "We have seen that multi-layered model is more *powerful* and *expressive*, than one-layered one. You may be wondering why don't we always use many-layered model. The answer to this question is **overfitting**.\n",
    "\n",
    "We will deal with this term more in a later sections, but the idea is the following: **the more powerful the model is, the better it can approximate training data, and the more data it needs to properly generalize** for the new data it has not seen before.\n",
    "\n",
    "**A linear model:**\n",
    "* We are likely to get high training loss - so-called **underfitting**, when the model does not have enough power to correctly separate all data. \n",
    "* Valiadation loss and training loss are more or less the same. The model is likely to generalize well to test data.\n",
    "\n",
    "**Complex multi-layered model**\n",
    "* Low training loss - the model can approximate training data well, because it has enough expressive power.\n",
    "* Validation loss can be much higher than training loss and can start to increase during training - this is because the model \"memorizes\" training points, and loses the \"overall picture\"\n",
    "\n",
    "![Overfitting](images/overfit.png)\n",
    "\n",
    "> On this picture, `x` stands for training data, `o` - validation data. Left - linear model (one-layer), it approximates the nature of the data pretty well. Right - overfitted model, the model perfectly well approximates training data, but stops making sense with any other data (validation error is very high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Takeaways\n",
    "\n",
    "* Simple models (fewer layers, fewer neurons) with low number of parameters (\"low capacity\") are less likely to overfit\n",
    "* More complex models (more layers, more neurons on each layer, high capacity) are likely to overfit. We need to monitor validation error to make sure it does not start to rise with further training\n",
    "* More complex models need more data to train on.\n",
    "* You can solve overfitting problem by either:\n",
    "    - simplifying your model\n",
    "    - increasing the amount of training data\n",
    "* **Bias-variance trade-off** is a term that shows that you need to get the compromise\n",
    "    - between power of the model and amount of data,\n",
    "    - between overfittig and underfitting\n",
    "* There is not single recipe on how many layers of parameters you need - the best way is to experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "\n",
    "This notebook is a part of [AI for Beginners Curricula](http://github.com/microsoft/ai-for-beginners), and has been prepared by [Dmitry Soshnikov](http://soshnikov.com). It is inspired by Neural Network Workshop at Microsoft Research Cambridge. Some code and illustrative materials are taken from presentations by [Katja Hoffmann](https://www.microsoft.com/en-us/research/people/kahofman/), [Matthew Johnson](https://www.microsoft.com/en-us/research/people/matjoh/) and [Ryoto Tomioka](https://www.microsoft.com/en-us/research/people/ryoto/), and from [NeuroWorkshop](http://github.com/shwars/NeuroWorkshop) repository."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "livereveal": {
   "start_slideshow_at": "selected"
  },
  "vscode": {
   "interpreter": {
    "hash": "d355c6d9fcfa2da36351d09a4957315c029537f44307b30fb3762ace87798487"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
